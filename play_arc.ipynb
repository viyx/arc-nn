{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ARCDataset, ColorPermutation, GPTDataset\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ARCDataset()\n",
    "no_aug_ds = GPTDataset(ds, 10, 2048, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "no_aug_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate lengths of GPTed samples\n",
    "gpt_lx, gpt_ly = [], []\n",
    "\n",
    "\n",
    "for id in range(len(ds)):\n",
    "    x_gpt, y_gpt =  no_aug_ds[id]\n",
    "    lxs.append(len(x_gpt))\n",
    "    lys.append(len(y_gpt))\n",
    "    \n",
    "    #check special token counts\n",
    "    x, y, x_test, y_test = ds[id]\n",
    "    assert (x_gpt == no_aug_ds.end_episode).sum() == len(x) + 1, \"End of episodes missmatched.\"\n",
    "    assert (x_gpt == no_aug_ds.promt).sum() == len(x) + 1, \"Promts missmatched.\"\n",
    "#     assert (x_gpt == no_aug_ds.new_line).sum() == len(x) + 1, \"Promts missmatched.\"\n",
    "    \n",
    "lxs, lys = pd.Series(lxs), pd.Series(lys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count     800.000000\n",
       "mean     2618.681250\n",
       "std      1139.220106\n",
       "min      2048.000000\n",
       "0%       2048.000000\n",
       "10%      2048.000000\n",
       "20%      2048.000000\n",
       "30%      2048.000000\n",
       "40%      2048.000000\n",
       "50%      2048.000000\n",
       "60%      2126.800000\n",
       "70%      2496.000000\n",
       "80%      2956.400000\n",
       "90%      4019.400000\n",
       "max      9310.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "lxs.describe(np.arange(0,1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count     458.0\n",
       "mean     2048.0\n",
       "std         0.0\n",
       "min      2048.0\n",
       "25%      2048.0\n",
       "50%      2048.0\n",
       "75%      2048.0\n",
       "max      2048.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# take only  ~half of smallest samples\n",
    "# round median to nearest power of 2 (1024 for us) \n",
    "\n",
    "t = lxs[lxs <= 2048].index.tolist()\n",
    "lxs[t].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "343 115\n"
     ]
    }
   ],
   "source": [
    "median_tasks = np.array(ds.tasks)[t].tolist()\n",
    "# median_tasks\n",
    "\n",
    "# #train : #test = 3 : 1 \n",
    "p1 = 3 * len(median_tasks) // 4\n",
    "p2 = len(median_tasks) - p1\n",
    "\n",
    "print(p1, p2)\n",
    "\n",
    "median_tasks_train = median_tasks[:p1]\n",
    "median_tasks_test = median_tasks[p1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = ColorPermutation(max_colors=10, max_permutations=100000)\n",
    "ds_train_median = ARCDataset(tasks=median_tasks_train, augs=[perm])\n",
    "ds_test_median = ARCDataset(tasks=median_tasks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20514980"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ds_train_median.pickle', 'wb') as f:\n",
    "    pickle.dump(ds_train_median, f)\n",
    "    \n",
    "with open('ds_test_median.pickle', 'wb') as f:\n",
    "    pickle.dump(ds_test_median, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ARCDataset, ColorPermutation\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    \"\"\"Flat 2D samples and add specials tokens.\n",
    "    \n",
    "    General scheme:\n",
    "    \n",
    "    flatten(x) + `promt` + flatten(y) + `end_episode`\n",
    "    \n",
    "    Here `flatten` is:\n",
    "    flat 2D array and add `end_line` in the end of every line.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ds, n_colors, n_context, padding=False):\n",
    "        self.ds = ds\n",
    "        self.n_colors = n_colors\n",
    "        self.n_context = n_context\n",
    "        self.padding = padding # expand x to n_context\n",
    "        self.max_y_size = 30 * 30 + 30 + 1 # max flatten y size with special tokens(end_lines, end_episode)\n",
    "        \n",
    "        # make special tokens \n",
    "        self.end_line = n_colors + 0    # array of shape (10, 3) has 10 end_lines\n",
    "        self.promt = n_colors + 1       # promt after every x\n",
    "        self.end_episode = n_colors + 2 # end of episode\n",
    "        self.pad = n_colors + 3       # padding token (expand to n_context from the begining)\n",
    "        \n",
    "        self.vocab_size = n_colors + 4\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def flat_all_sample(self, x, y, x_test, y_test):\n",
    "        \n",
    "        def flat_2D_field(x):\n",
    "            \"Add column of `end_line` tokens and flat 2D array.\"\n",
    "            a = np.array([[self.end_line]] * x.shape[0], dtype=np.long)\n",
    "            a = np.hstack([x, a])\n",
    "            return a.ravel()\n",
    "    \n",
    "        def flat_pair(pair):\n",
    "            \"Flat x,y pairs and add `promt` and `end_episode` tokens\"\n",
    "            x, y = pair\n",
    "            x = flat_2D_field(x)\n",
    "            y = flat_2D_field(y)\n",
    "            return np.concatenate([x, self.promt, y, self.end_episode], axis=None)\n",
    "        \n",
    "        def pad(seq, to, direct):\n",
    "            \"Pad sequence to left ot right.\"\n",
    "            x = np.array([self.pad] * to, dtype=np.long)\n",
    "            if(direct == 'left'):\n",
    "                x[-len(seq):] = seq\n",
    "            if(direct == 'right'):\n",
    "                x[:len(seq)] = seq\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        # flat train pairs\n",
    "        xy = list(map(flat_pair, zip(x, y)))\n",
    "        xy = np.concatenate(xy, axis=None)\n",
    "        \n",
    "        #flat test pair\n",
    "        \n",
    "        # take only the first test episode (may be >1)\n",
    "        # if we keep all test episodes batching would be harder to control\n",
    "#         if(len(x_test) > 1):\n",
    "        x_test = x_test[0]\n",
    "        y_test = y_test[0]\n",
    "            \n",
    "        xt = flat_2D_field(x_test)\n",
    "        yt = flat_2D_field(y_test)\n",
    "        \n",
    "        # just add end of episode\n",
    "        y = np.concatenate([yt, self.end_episode], axis=None)\n",
    "        \n",
    "        # pad y to max flattened 2D field\n",
    "        max_tokens_on_field = 30**2 + 30 + 1\n",
    "        if(len(y) < max_tokens_on_field and self.padding):\n",
    "            y = pad(y, max_tokens_on_field, 'right')\n",
    "        \n",
    "        # context: concat all\n",
    "        x = np.concatenate([xy, xt, self.promt, y], axis=None)\n",
    "        \n",
    "        # padding\n",
    "        if(len(x) < self.n_context and self.padding): # expand sample to n_context\n",
    "            x = pad(x, self.n_context, 'left')\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        x, y, x_test, y_test = self.ds[id]\n",
    "        x, y = self.flat_all_sample(x, y, x_test, y_test)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ds_train_median.pickle', 'rb') as f:\n",
    "    ds_train_median = pickle.load(f)\n",
    "    \n",
    "with open('ds_test_median.pickle', 'rb') as f:\n",
    "    ds_test_median = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GPTDataset(ds_train_median, 10, 2048, padding=True)\n",
    "test_dataset = GPTDataset(ds_test_median, 10, 2048, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20514980 115\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.ds.aug_tasks.loc[(1, slice(None)),][0].values[0][14][1][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# we'll do something a bit smaller\n",
    "mconf = GPTConfig(train_dataset.vocab_size, block_size=train_dataset.n_context,\n",
    "                  masked_length = 30 ** 2 + 30 + 1, padding_idx=13,\n",
    "                  embd_pdrop=0.0, resid_pdrop=0.1, attn_pdrop=0.1,\n",
    "                  n_layer=12, n_head=8, n_embd=256)\n",
    "model = GPT(mconf)\n",
    "\n",
    "tokens_per_epoch = len(train_dataset) * 576 #mean x length\n",
    "train_epochs = 1 # todo run a bigger model and longer, this is tiny\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=train_epochs, batch_size=128, learning_rate=3e-3,\n",
    "                      betas = (0.9, 0.95), weight_decay=0,\n",
    "                      lr_decay=True, warmup_tokens=tokens_per_epoch, final_tokens=train_epochs*tokens_per_epoch,\n",
    "                      ckpt_path='model.pt',\n",
    "                      num_workers=2, early_stopping=1000)\n",
    "\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/160274 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2048) must match the size of tensor b (262144) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/arc-nn/mingpt/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/arc-nn/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m# forward the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# collapse all losses if they are scattered on multiple gpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/arc-nn/mingpt/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m#set padding index for positions embeddings as in token embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (262144) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}