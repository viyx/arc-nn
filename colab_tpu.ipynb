{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PyTorch on Cloud TPUs: MultiCore Training AlexNet on Fashion MNIST",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viyx/arc-nn/blob/tpu/colab_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZPes1I3H8l-"
      },
      "source": [
        "!rm -rf arc-nn\n",
        "!git clone -q https://github.com/viyx/arc-nn -b tpu\n",
        "%cd arc-nn\n",
        "!git log --pretty=oneline\n",
        "!wget -q https://storage.googleapis.com/viy_data/pickle/median/ds_test_median.pickle\n",
        "!wget -q https://storage.googleapis.com/viy_data/pickle/median/ds_train_median.pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3A-D5MFIUYa"
      },
      "source": [
        "import pickle\n",
        "from datasets import ARCDataset, ColorPermutation, GPTDataset\n",
        "from mingpt.utils import set_seed\n",
        "\n",
        "# flags = {'seed': 19}\n",
        "\n",
        "# set_seed(flags['seed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCX6ebMRizuh"
      },
      "source": [
        "#create minidataset for testing\n",
        "\n",
        "# ds = ARCDataset(data_folder='./arc-nn/data')\n",
        "# perm = ColorPermutation(max_colors=10, max_permutations=100000)\n",
        "# ds_train = ARCDataset(tasks=ds.tasks[:10], augs=[perm])\n",
        "# ds_test = ARCDataset(tasks=ds.tasks[10:39])\n",
        "\n",
        "# with open('train_dataset.pickle', 'wb') as f:\n",
        "#   pickle.dump(ds_train, f)\n",
        "    \n",
        "# with open('test_dataset.pickle', 'wb') as f:\n",
        "#   pickle.dump(ds_test, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAzIwJg2vweI"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UYwdM3qRjhS"
      },
      "source": [
        "### Installing PyTorch/XLA\n",
        "\n",
        "Run the following cell (or copy it into your own notebook!) to install PyTorch, Torchvision, and PyTorch/XLA. It will take a couple minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imKiCg3wReXn"
      },
      "source": [
        "# Installs PyTorch, PyTorch/XLA, and Torchvision\n",
        "# Copy this cell into your own notebooks to use PyTorch on Cloud TPUs \n",
        "# Warning: this may take a couple minutes to run\n",
        "!pip install -q cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqx-VgEizPiF"
      },
      "source": [
        "import torch\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import time\n",
        "import tqdm\n",
        "from mingpt.model import GPT, GPTConfig\n",
        "from mingpt.trainer import TrainerConfig\n",
        "\n",
        "def map_fn(index, flags):\n",
        "  ## Setup \n",
        "\n",
        "  # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "  set_seed(flags['seed'])\n",
        "\n",
        "  # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "  device = xm.xla_device()  \n",
        "\n",
        "  if not xm.is_master_ordinal():\n",
        "    xm.rendezvous('download_only_once')\n",
        "\n",
        "  with open('ds_train_median.pickle', 'rb') as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "        \n",
        "  with open('ds_test_median.pickle', 'rb') as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "  train_dataset = GPTDataset(train_dataset, n_colors=10, n_context=2048, padding=True)\n",
        "  test_dataset = GPTDataset(test_dataset,  n_colors=10, n_context=2048, padding=True)\n",
        "\n",
        "  print(len(train_dataset), len(test_dataset))\n",
        "  mconf = GPTConfig(train_dataset.vocab_size, block_size=train_dataset.n_context,\n",
        "                    masked_length = 30 ** 2 + 30 + 1, padding_idx=13,\n",
        "                    embd_pdrop=0.0, resid_pdrop=0.1, attn_pdrop=0.1,\n",
        "                    n_layer=2, n_head=8, n_embd=8)\n",
        "\n",
        "\n",
        "#   train(model, train_dataset, test_dataset, tconf)\n",
        "  \n",
        "  if xm.is_master_ordinal():\n",
        "    xm.rendezvous('download_only_once')\n",
        "  \n",
        "  # Creates the (distributed) train sampler, which let this process only access\n",
        "  # its portion of the training dataset.\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  \n",
        "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    test_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "  \n",
        "  # Creates dataloaders, which load data in batches\n",
        "  # Note: test loader is not shuffled or sampled\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=test_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "  \n",
        "\n",
        "  model = GPT(mconf).to(device).train()\n",
        "\n",
        "  tokens_per_epoch = len(train_dataset) * 576 #mean x length\n",
        "  train_epochs = 1\n",
        "\n",
        "  tconf = TrainerConfig(max_epochs=1, batch_size=128, learning_rate=3e-3,\n",
        "                      betas = (0.9, 0.95), weight_decay=0,\n",
        "                      lr_decay=True, warmup_tokens=tokens_per_epoch, \n",
        "                      final_tokens=train_epochs*tokens_per_epoch,\n",
        "                      ckpt_path='model.pt',\n",
        "                      num_workers=2, early_stopping=1000)\n",
        "\n",
        "  optimizer = model.configure_optimizers(tconf)\n",
        "\n",
        "  ## Trains\n",
        "  train_start = time.time()\n",
        "  for epoch in range(flags['num_epochs']):\n",
        "    para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "    pbar = tqdm(enumerate(para_train_loader), total=len(para_train_loader))\n",
        "\n",
        "    for it, batch in pbar:\n",
        "      x, y = batch\n",
        "\n",
        "      logits, loss = model(x, y)\n",
        "      loss = loss.mean()\n",
        "\n",
        "      # Updates model\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      xm.optimizer_step(optimizer)  # Note: barrier=True not needed when using ParallelLoader \n",
        "      pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}.\")\n",
        "\n",
        "  elapsed_train_time = time.time() - train_start\n",
        "  print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time) \n",
        "\n",
        "\n",
        "  ## Evaluation\n",
        "  # Sets net to eval and no grad context \n",
        "#   net.eval()\n",
        "#   eval_start = time.time()\n",
        "#   with torch.no_grad():\n",
        "#     num_correct = 0\n",
        "#     total_guesses = 0\n",
        "\n",
        "#     para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "#     for batch_num, batch in enumerate(para_train_loader):\n",
        "#       data, targets = batch\n",
        "\n",
        "#       # Acquires the network's best guesses at each class\n",
        "#       output = net(data)\n",
        "#       best_guesses = torch.argmax(output, 1)\n",
        "\n",
        "#       # Updates running statistics\n",
        "#       num_correct += torch.eq(targets, best_guesses).sum().item()\n",
        "#       total_guesses += flags['batch_size']\n",
        "  \n",
        "  elapsed_eval_time = time.time() - eval_start\n",
        "  print(\"Process\", index, \"finished evaluation. Evaluation time was:\", elapsed_eval_time)\n",
        "  print(\"Process\", index, \"guessed\", num_correct, \"of\", total_guesses, \"correctly for\", num_correct/total_guesses * 100, \"% accuracy.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rDe9KaS1mzz"
      },
      "source": [
        "# Configures training (and evaluation) parameters\n",
        "flags = {}\n",
        "flags['batch_size'] = 8\n",
        "flags['num_workers'] = 8\n",
        "flags['num_epochs'] = 1\n",
        "flags['seed'] = 1234\n",
        "\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSp0EOq8Pg2b"
      },
      "source": [
        "##What's Next?\n",
        "\n",
        "This notebook broke down training AlexNet on the Fashion MNIST dataset using an entire Cloud TPU. A [previous notebook](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/single-core-alexnet-fashion-mnist.ipynb) showed how to train AlexNet on Fashion MNIST using only a single Cloud TPU core, and can be a helpful point of comparison. \n",
        "\n",
        "In particular, this notebook showed us how to:\n",
        "\n",
        "- Define a \"map function\" that runs in parallel on one process per Cloud TPU core. \n",
        "- Run the map function using `spawn`.\n",
        "- Understand the Cloud TPU context, its benefits, like automatic cross-process coordination, and its limits, like needing each process to perform the same Cloud TPU operations.\n",
        "- Load and sample the datasets.\n",
        "- Train and evaluate the network.\n",
        "\n",
        "Additional notebooks demonstrating how to run PyTorch on Cloud TPUs can be found [here](https://github.com/pytorch/xla). While Colab provides a free Cloud TPU, training is even faster on [Google Cloud Platform](https://github.com/pytorch/xla#Cloud), especially when using multiple Cloud TPUs in a Cloud TPU pod. Scaling from a single Cloud TPU, like in this notebook, to many Cloud TPUs in a pod is easy, too. You use the same code as this notebook and just spawn more processes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tey7cpCppIFV"
      },
      "source": [
        "!ls -lh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5NvP0ZPpN79"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f90_xXSljzEe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJgIF6cbMhz0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}